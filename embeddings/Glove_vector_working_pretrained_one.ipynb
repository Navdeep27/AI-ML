{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5omHLLytMX3u"
      },
      "outputs": [],
      "source": [
        "# code for Glove word embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_for_vocab(filepath, word_index,\n",
        "\t\t\t\t\t\tembedding_dim):\n",
        "\tvocab_size = len(word_index) + 1\n",
        "\n",
        "\t# Adding again 1 because of reserved 0 index\n",
        "\tembedding_matrix_vocab = np.zeros((vocab_size,\n",
        "\t\t\t\t\t\t\t\t\tembedding_dim))\n",
        "\n",
        "\twith open(filepath, encoding=\"utf8\") as f:\n",
        "\t\tfor line in f:\n",
        "\t\t\tword, *vector = line.split()\n",
        "\t\t\tif word in word_index:\n",
        "\t\t\t\tidx = word_index[word]\n",
        "\t\t\t\tembedding_matrix_vocab[idx] = np.array(\n",
        "\t\t\t\t\tvector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "\treturn embedding_matrix_vocab"
      ],
      "metadata": {
        "id": "-kJXrQmwN1wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = {'text', 'the', 'leader', 'prime',\n",
        "\t'natural', 'language'}\n",
        "\n",
        "# create the dict.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x)\n",
        "\n",
        "# number of unique words in dict.\n",
        "print(\"Number of unique words in dictionary=\",\n",
        "\tlen(tokenizer.word_index))\n",
        "print(\"Dictionary is = \", tokenizer.word_index)\n",
        "\n",
        "\n",
        "\n",
        "# matrix for vocab: word_index\n",
        "embedding_dim = 50\n",
        "embedding_matrix_vocab = embedding_for_vocab(\n",
        "\t'/content/sample_data/glove.6B.50d.txt', tokenizer.word_index,\n",
        "embedding_dim)\n",
        "\n",
        "print(\"Dense vector for first word is => \",\n",
        "\tembedding_matrix_vocab[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-EXNV2lMctR",
        "outputId": "f960c8d2-4495-4937-9f35-5ace1ff39538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dictionary= 6\n",
            "Dictionary is =  {'prime': 1, 'language': 2, 'the': 3, 'natural': 4, 'leader': 5, 'text': 6}\n",
            "Dense vector for first word is =>  [ 0.50795001  0.69881999  0.41468     0.49972999  0.82731003  0.58882999\n",
            " -0.43408     0.21703    -1.81809998 -0.74273998 -0.17991     0.28492999\n",
            " -0.16937     0.87449002  0.55294001  0.91030997  0.21957    -0.4851\n",
            "  0.75489002  0.52341998  0.5438      0.10108    -0.07919    -0.11478\n",
            "  0.29473999 -1.60039997  0.52854002  0.04084    -0.7198      1.93540001\n",
            "  2.81900001  0.60715997 -1.12080002  0.057194    0.14309999  0.47372001\n",
            "  0.59581     0.11381    -0.79955    -0.28086999 -0.32896999  1.32560003\n",
            " -1.18040001 -1.38600004  0.20202     0.51486999 -1.90680003  0.65419\n",
            "  1.72459996 -0.6013    ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download glove and unzip it in Notebook.\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip glove*.zip\n",
        "\n",
        "# vocab: 'the': 1, mapping of words with\n",
        "# integers in seq. 1,2,3..\n",
        "# embedding: 1->dense vector\n"
      ],
      "metadata": {
        "id": "ZTdStXv0Mhvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix for vocab: word_index\n",
        "embedding_dim = 50\n",
        "embedding_matrix_vocab = embedding_for_vocab(\n",
        "    '/content/sample_data/glove.6B.50d.txt', tokenizer.word_index,\n",
        "  embedding_dim)\n",
        "\n",
        "print(\"Dense vector for first word is => \",\n",
        "      embedding_matrix_vocab[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRplGbRmM_dg",
        "outputId": "47117e8d-5f86-4782-a584-33eb8ec6d69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense vector for first word is =>  [ 0.50795001  0.69881999  0.41468     0.49972999  0.82731003  0.58882999\n",
            " -0.43408     0.21703    -1.81809998 -0.74273998 -0.17991     0.28492999\n",
            " -0.16937     0.87449002  0.55294001  0.91030997  0.21957    -0.4851\n",
            "  0.75489002  0.52341998  0.5438      0.10108    -0.07919    -0.11478\n",
            "  0.29473999 -1.60039997  0.52854002  0.04084    -0.7198      1.93540001\n",
            "  2.81900001  0.60715997 -1.12080002  0.057194    0.14309999  0.47372001\n",
            "  0.59581     0.11381    -0.79955    -0.28086999 -0.32896999  1.32560003\n",
            " -1.18040001 -1.38600004  0.20202     0.51486999 -1.90680003  0.65419\n",
            "  1.72459996 -0.6013    ]\n"
          ]
        }
      ]
    }
  ]
}